{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d24a4f",
   "metadata": {},
   "source": [
    "# Praktikum Week 8 Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a74ea",
   "metadata": {},
   "source": [
    "## Muhammad Naufal Ilman\n",
    "## 162012133067\n",
    "## Link Github: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0872b",
   "metadata": {},
   "source": [
    "## 1. Dengan menggunakan Python Requests dan BeautifulSoup cobalah untuk scraping 1 halaman website unair news (https://unair.ac.id/news) dan judul berita yang ada hari ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f9e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library yang diperlukan\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e8d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t\t\t\tWebsvaganza 2023 Hadirkan Bazar Kosmetik untuk Dorong Kepercayaan Diri dan Hilangkan Insecure\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa UNAIR Sabet Juara 1 Kategori Lomba Infografis KOMINFO Jawa Timur\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tPentingnya Mengenal Potensi Diri Melalui Pemahaman Emosional\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDukung Pendidikan Merata UNAIR Berikan Beasiswa Siswa Sekolah Dasar\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tApa Sih UKM Wanala ?\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDelegasi FH Kembali Juarai Lomba Debat Diponegoro Law Fair\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tMahasiswa FH UNAIR Sabet Juara III Lomba Paper Internasional\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tPakar Hukum Humaniter Internasional UNAIR Sebut Tindakan Israel Merupakan Crime Against Humanity\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tAncam Boikot SpaceX, Dosen UNAIR Sebut Israel Tidak Ingin Aksi Genosida Diketahui Dunia Luar\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tKunjungan Resmi Wufeng University, Sepakati Berbagai MoU dengan UNAIR\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tDr Andriyanto, Alumnus UNAIR yang Dilantik Menjadi Pj Bupati Pasuruan\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tTaiwan Higher Education Fair Tawarkan Beragam Beasiswa Kuliah\t\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# halaman berita unair yang akan di scraping\n",
    "url = \"https://unair.ac.id/category/berita/\"\n",
    "# mengirim permintaan HTTP GET ke url\n",
    "response = requests.get(url)\n",
    "# mengambil response dalam bentuk format HTML, kemudian disimpan dalam variabel \n",
    "rawhtml = response.text\n",
    "# melakukan parsing pada HTML\n",
    "soup = BeautifulSoup(rawhtml, 'html.parser')\n",
    "for i in soup.find_all('h3'):\n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a7f9d",
   "metadata": {},
   "source": [
    "## 2.\tLalu, lakukan crawling unair news dengan mengkombinasikan Python Request, beautifulsoup, dan for loop, untuk mengambil judul berita dari kategori featured news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2049f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libary yang diperlukan\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b8153a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to scraped_data.json\n"
     ]
    }
   ],
   "source": [
    "# Define the initial URL\n",
    "base_url = 'https://unair.ac.id/category/featured/'\n",
    "next_page = base_url\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop until there are no more \"next\" links\n",
    "while next_page:\n",
    "    # Send an HTTP GET request to the current page\n",
    "    response = requests.get(next_page)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the data you need from the current page\n",
    "        # Replace this with your specific scraping logic\n",
    "        # For example, if the titles are inside <h3> elements:\n",
    "        titles = soup.find_all('h3')\n",
    "        for title in titles:\n",
    "            title_text = title.get_text().strip()  # Remove leading/trailing whitespace and newline characters\n",
    "            results.append(title_text)\n",
    "\n",
    "        # Find the link to the next page\n",
    "        next_page_tag = soup.find('link', rel='next')\n",
    "        if next_page_tag:\n",
    "            next_page = next_page_tag.get('href')\n",
    "        else:\n",
    "            next_page = None  # No more \"next\" link\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('scraped_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data has been saved to scraped_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c67905",
   "metadata": {},
   "source": [
    "## 3. Setelah itu, gunakan scrapy untuk melakukan crawling website https://bit.ly/scrapingtry dan menyimpan judul game beserta harganya dalam tabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf972206",
   "metadata": {},
   "source": [
    "link website: https://store.playstation.com/en-id/category/05a2d027-cedc-4ac0-abeb-8fc26fec7180/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16083580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "from scrapy.spiders import Spider\n",
    "class GamePricesSpider(scrapy.Spider):\n",
    "    name = 'game_prices'\n",
    "    start_urls = ['https://store.playstation.com/en-id/category/05a2d027-cedc-4ac0-abeb-8fc26fec7180/1']\n",
    "\n",
    "    def __init__(self, max_pages=16, *args, **kwargs):\n",
    "        super(GamePricesSpider, self).__init__(*args, **kwargs)\n",
    "        self.max_pages = max_pages\n",
    "\n",
    "    def parse(self, response):\n",
    "        games = response.css('.psw-t-body.psw-c-t-1.psw-t-truncate-2.psw-m-b-2')\n",
    "        prices = response.css('.psw-m-r-3')\n",
    "\n",
    "        # Open the CSV file with proper encoding\n",
    "        with open('scraped_data.csv', 'a', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            if response.url.endswith('/1'):\n",
    "                writer.writerow(['title', 'price'])  # Write the header\n",
    "            for game, price in zip(games, prices):\n",
    "                title = game.css('span::text').get()\n",
    "                game_price = price.css('span::text').get()\n",
    "                # Remove the non-breaking space (U+00A0) characters\n",
    "                title = title.replace(u'\\u00A0', u'')\n",
    "                game_price = game_price.replace(u'\\u00A0', u'')\n",
    "                writer.writerow([title, game_price])\n",
    "\n",
    "        # Generate the next page URL based on the pattern\n",
    "        self.page_number = int(response.url.split('/')[-1]) + 1\n",
    "        next_page_url = f'https://store.playstation.com/en-id/category/05a2d027-cedc-4ac0-abeb-8fc26fec7180/{self.page_number}'\n",
    "\n",
    "        # Follow the next page URL\n",
    "        if self.page_number <= self.max_pages:\n",
    "            yield response.follow(next_page_url, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40868d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
